# 多元化奖励函数系统使用指南

## 概述

本系统实现了一个基于多元化奖励函数的强化学习无人机导航系统，旨在训练无人机具备以下核心能力：

1. **循迹能力** - 按照预定航线精确飞行
2. **寻回定位能力** - 飞出预定管道后能够自主返回航线
3. **紧急决策能力** - 在电池不足等紧急情况下做出最优决策
4. **安全飞行能力** - 避免碰撞和违规操作
5. **能效优化能力** - 在满足任务要求的前提下优化能耗

## 系统架构

### 核心组件

```
├── src/
│   ├── environment/
│   │   ├── airsim_env.py          # 主环境类
│   │   └── reward_function.py     # 多元化奖励函数
│   └── training/
│       └── train_with_multidimensional_reward.py  # 训练脚本
├── configs/
│   └── multidimensional_reward_training.yaml      # 训练配置
└── docs/
    ├── 强化学习奖励函数设计分析.md
    └── 多元化奖励函数系统使用指南.md
```

### 奖励函数组件

#### 1. 循迹能力奖励 (TrackingReward)

**目标**: 训练无人机精确跟随预定航线

**计算方式**:
- 距离奖励: 基于与航线的偏离距离
- 航向一致性: 飞行方向与期望方向的一致性
- 速度一致性: 实际速度与期望速度的匹配度

```python
# 距离奖励
distance_reward = max(0, 1 - deviation / max_deviation)

# 航向一致性奖励
heading_reward = max(0, cos(heading_error))

# 速度一致性奖励
speed_reward = max(0, 1 - abs(actual_speed - expected_speed) / expected_speed)
```

#### 2. 寻回定位能力奖励 (RecoveryReward)

**目标**: 训练无人机在偏离航线后快速返回

**触发条件**: 当无人机偏离航线超过阈值时激活

**计算方式**:
- 寻回进度奖励: 基于返回航线的进度
- 时间效率奖励: 基于寻回所用时间
- 成功/失败奖励: 寻回成功给予奖励，失败给予惩罚

#### 3. 紧急决策能力奖励 (EmergencyReward)

**目标**: 训练无人机在紧急情况下做出最优决策

**触发条件**: 
- 电池电量低于临界阈值
- 遇到恶劣天气
- 系统故障等

**决策评估**:
- 能效优化: 选择最节能的路径
- 决策质量: 基于当前状态的最优性评估
- 安全考量: 确保决策不违反安全约束

#### 4. 安全性奖励 (SafetyReward)

**目标**: 确保飞行安全

**监控项目**:
- 碰撞检测
- 高度违规
- 速度违规
- 接近危险区域

#### 5. 能效奖励 (EfficiencyReward)

**目标**: 优化能源使用效率

**评估指标**:
- 能耗率
- 时间效率
- 路径优化程度

## 使用方法

### 1. 环境准备

```bash
# 安装依赖
pip install stable-baselines3 airsim gymnasium numpy pyyaml

# 启动AirSim
# 确保AirSim已正确安装并运行
```

### 2. 配置文件设置

编辑 `configs/multidimensional_reward_training.yaml` 文件，根据需要调整参数：

```yaml
# 奖励权重配置
reward_function:
  weights:
    tracking: 0.3      # 循迹能力权重
    recovery: 0.2      # 寻回能力权重
    emergency: 0.2     # 紧急决策权重
    safety: 0.2        # 安全性权重
    efficiency: 0.1    # 能效权重
```

### 3. 开始训练

```bash
# 进入项目目录
cd /home/wdblink/repo/pilot_rl_navigation

# 运行训练脚本
python src/training/train_with_multidimensional_reward.py
```

### 4. 监控训练过程

```bash
# 启动TensorBoard监控
tensorboard --logdir=./logs/tensorboard
```

访问 `http://localhost:6006` 查看训练进度和指标。

## 课程学习策略

系统采用渐进式训练策略，分为四个阶段：

### 阶段1: 基础循迹训练 (0-50k steps)
- **重点**: 循迹能力和基础安全
- **环境**: 简单环境，无干扰
- **权重**: 循迹(0.4) + 安全(0.6)

### 阶段2: 寻回能力训练 (50k-125k steps)
- **重点**: 增加寻回能力训练
- **环境**: 中等复杂度，引入轻微干扰
- **权重**: 循迹(0.3) + 寻回(0.4) + 安全(0.3)

### 阶段3: 紧急决策训练 (125k-225k steps)
- **重点**: 紧急情况处理
- **环境**: 复杂环境，电池限制
- **权重**: 紧急(0.4) + 安全(0.3) + 能效(0.3)

### 阶段4: 综合能力训练 (225k+ steps)
- **重点**: 所有能力综合训练
- **环境**: 全复杂度环境
- **权重**: 平衡所有组件

## 评估指标

### 核心指标

1. **循迹精度** (Tracking Accuracy)
   - 平均偏离距离
   - 航线完成率
   - 航向一致性

2. **寻回成功率** (Recovery Success Rate)
   - 寻回成功次数 / 偏离次数
   - 平均寻回时间
   - 寻回路径效率

3. **紧急决策准确性** (Emergency Decision Accuracy)
   - 最优决策选择率
   - 紧急情况处理成功率
   - 能效优化程度

4. **安全性指标** (Safety Metrics)
   - 碰撞次数
   - 违规操作次数
   - 安全边际维持率

5. **能效指标** (Efficiency Metrics)
   - 平均能耗
   - 任务完成时间
   - 路径优化度

### 评估脚本

```python
# 运行评估
from src.training.train_with_multidimensional_reward import MultiDimensionalRewardTrainer

trainer = MultiDimensionalRewardTrainer(config_path="configs/multidimensional_reward_training.yaml")
results = trainer.evaluate_model(model_path="models/best_model.zip", n_episodes=100)
print(f"评估结果: {results}")
```

## 超参数调优建议

### 奖励权重调优

1. **初始阶段**: 重点关注安全性和基础循迹
   ```yaml
   weights:
     tracking: 0.4
     safety: 0.4
     others: 0.2
   ```

2. **中期阶段**: 平衡各项能力
   ```yaml
   weights:
     tracking: 0.25
     recovery: 0.25
     emergency: 0.2
     safety: 0.2
     efficiency: 0.1
   ```

3. **后期阶段**: 根据具体应用场景调整

### 学习率调优

- **初始学习率**: 3e-4
- **学习率衰减**: 使用线性衰减
- **最小学习率**: 1e-5

### 网络架构调优

```python
# 推荐网络架构
policy_kwargs = dict(
    net_arch=[dict(pi=[256, 256], vf=[256, 256])],
    activation_fn=torch.nn.ReLU
)
```

## 故障排除

### 常见问题

1. **训练不收敛**
   - 检查奖励函数权重设置
   - 降低学习率
   - 增加探索系数

2. **AirSim连接失败**
   - 确认AirSim正在运行
   - 检查IP和端口配置
   - 重启AirSim

3. **内存不足**
   - 减少并行环境数量
   - 降低批次大小
   - 使用梯度累积

4. **奖励函数异常**
   - 检查状态数据完整性
   - 验证奖励计算逻辑
   - 添加调试日志

### 调试技巧

```python
# 启用详细日志
import logging
logging.basicConfig(level=logging.DEBUG)

# 奖励组件分析
reward_components = env.reward_function.get_last_reward_breakdown()
print(f"奖励组件: {reward_components}")

# 状态可视化
env.render_state_info()
```

## 扩展和定制

### 添加新的奖励组件

1. 在 `reward_function.py` 中创建新的奖励类
2. 继承 `BaseReward` 类
3. 实现 `calculate` 方法
4. 在 `MultiDimensionalRewardFunction` 中注册

```python
class CustomReward(BaseReward):
    def calculate(self, state: FlightState, action: np.ndarray, 
                 next_state: FlightState, info: Dict) -> float:
        # 实现自定义奖励逻辑
        return reward_value
```

### 修改动作空间

在 `airsim_env.py` 中修改动作空间定义：

```python
# 扩展动作空间
self.action_space = spaces.Box(
    low=-1.0, high=1.0, 
    shape=(new_action_dim,), 
    dtype=np.float32
)
```

### 添加新的状态信息

在 `FlightState` 类中添加新的状态变量：

```python
@dataclass
class FlightState:
    # 现有状态变量...
    
    # 新增状态变量
    custom_sensor_data: np.ndarray = None
    environmental_conditions: Dict = None
```

## 性能优化

### 训练加速

1. **使用GPU**: 设置 `device: "cuda"`
2. **并行环境**: 增加 `n_envs` 数量
3. **批次优化**: 调整 `batch_size` 和 `n_steps`
4. **网络剪枝**: 使用较小的网络架构

### 内存优化

1. **状态缓存**: 限制历史状态存储
2. **梯度累积**: 使用梯度累积减少内存使用
3. **数据类型**: 使用 float32 而非 float64

## 部署和应用

### 模型导出

```python
# 导出训练好的模型
model = PPO.load("models/best_model.zip")
model.save("models/deployment_model")

# 转换为ONNX格式（可选）
import torch
torch.onnx.export(model.policy, dummy_input, "model.onnx")
```

### 实时推理

```python
# 实时推理示例
model = PPO.load("models/best_model.zip")
obs = env.reset()

while True:
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, done, info = env.step(action)
    
    if done:
        obs = env.reset()
```

## 结论

本多元化奖励函数系统通过综合考虑循迹、寻回、紧急决策、安全性和能效等多个维度，能够训练出具备全面飞行能力的强化学习无人机导航系统。通过合理的配置和调优，该系统能够在复杂环境中实现高效、安全的自主导航。

## 参考资料

- [Stable Baselines3 文档](https://stable-baselines3.readthedocs.io/)
- [AirSim 文档](https://microsoft.github.io/AirSim/)
- [强化学习奖励函数设计分析](./强化学习奖励函数设计分析.md)

---

**版本**: v1.0  
**更新日期**: 2024年  
**维护者**: 强化学习导航系统开发团队